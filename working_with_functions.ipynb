{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee571e87",
   "metadata": {},
   "source": [
    "### Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# %pip install openai==0.28\n",
    "# %pip install --upgrade langchain\n",
    "# %pip install --upgrade pydantic\n",
    "# %pip install --upgrade typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be99f26-d5df-4a87-b568-9f8cbca3f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import langchain\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d108b5-d04b-4f6f-84b0-1039f0e44cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store OpenAI API key in the venv\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('/Users/robertford/Documents/OpenAI-API/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "    for key in config:\n",
    "        os.environ['OPENAI_API_KEY'] = config[key]\n",
    "\n",
    "api_key=os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# print(os.environ['OPENAI_API_KEY']) # Check that it imported the key ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc8676-6142-4411-89ad-8b4da3e72948",
   "metadata": {},
   "source": [
    "### Extract information into a structured output using a Pydantic (data validation) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd4c8d-9b2c-458b-97ed-da73217e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class inheriting from the BaseModel class and define all the fields (arguments):\n",
    "\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class RequestStructure(BaseModel):\n",
    "  \"\"\"extracts information\"\"\"\n",
    "  metric: str = Field(description = \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\")\n",
    "  filters: Optional[str] = Field(description = \"filters to apply to the calculation (do not include filters on dates here)\")\n",
    "  dimensions: Optional[str] = Field(description = \"parameters to split you metric by\")\n",
    "  period_start: Optional[str] = Field(description = \"start day of the period for report\")\n",
    "  period_end: Optional[str] = Field(description = \"end day of the period for report\")\n",
    "  output_type: Optional[str] = Field(description = \"the desired output\", enum = [\"number\", \"visualisation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b52e93-ef79-4cba-a54a-0be03253653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain to convert this Pydantic class into an OpenAI function:\n",
    "\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "extract_info_function = convert_pydantic_to_openai_function(RequestStructure, name = 'extract_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181e0ed-5331-4c6c-aaa7-bcc51810be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain validates the class needed to use OpenAI:\n",
    "\n",
    "extract_info_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e23fc-1ceb-43a2-9cdf-334a996a1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a LangChain chain\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information from the provided request. \\\n",
    "            Extract ONLY the information presented in the initial request. \\\n",
    "            Don't add anything else. \\\n",
    "            Return partial information if something is missing.\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [extract_info_function])\n",
    "\n",
    "extraction_chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939e35f-620f-4af8-afff-dcc2cf8418ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke 'request' defined in the chain into a structured ouput:\n",
    "\n",
    "extraction_chain.invoke({'request': \"How many customers visited our site on iOS in April 2023 from different countries?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01dbf58-52a2-46af-b4a6-13a4ab76fa63",
   "metadata": {},
   "source": [
    "### Define a tool that performs an explicit calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad8b00-e29c-426d-8d5c-688c17b938b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teach the LLM data analyst to calculate the difference between two metrics using an explicit calculation.\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a6325-7cc0-43e9-a166-7251363791bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the function has the name and description parameters to be passed to the LLM:\n",
    "print(percentage_difference.name)\n",
    "print(percentage_difference.args)\n",
    "print(percentage_difference.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c682f11-2fbc-4237-ab2d-70f4716bd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above parameters to create an OpenAI function spec:\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "# format_tool_to_openai_function(percentage_difference)\n",
    "format_tool_to_openai_function(percentage_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70958fde-3dcf-4d6f-974b-631c12c9c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pydantic to specify a schema for the arguments:\n",
    "import pydantic.v1\n",
    "from typing import Type\n",
    "\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from pydantic.v1 because there is an error if using just pydantic\n",
    "# Use Pyndantic to specify a schema:\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "class Metrics(BaseModel):\n",
    "    metric1: float = Field(description=\"Base metric value to calculate the difference\")\n",
    "    metric2: float = Field(description=\"New metric value that we compare with the baseline\")\n",
    "\n",
    "@tool(args_schema=Metrics)\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2640cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a tool in practice to defining a chain and passing our tool to the function\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts.\"),\n",
    "    (\"user\", \"{request}\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model\n",
    "analyst_chain.invoke({'request': \"In April we had 100 users and in May only 95. What is difference in percent?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = analyst_chain.invoke({'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f69594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function as the LLM requested like this\n",
    "\n",
    "observation = percentage_difference(result.tool_input)\n",
    "print(observation)\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36934f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a message list to pass to the model observations to get a final answer from the model:\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "\n",
    "observation = percentage_difference(result1.tool_input)\n",
    "print(observation)\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we need to add the observation to our observations variable. We could use format_to_openai_functions function to format our results in an expected way for the model.\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s invoke our chain one more time, passing the function execution result as an observation.\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation)])\n",
    "})\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00432203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s add a couple more tools to our analyst’s toolkit,\n",
    "# using Pydantic to specify the input arguments for our function.\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "class Filters(BaseModel):\n",
    "    month: str = Field(description=\"Month of customer's activity in the format %Y-%m-%d\")\n",
    "    city: Optional[str] = Field(description=\"City of residence for customers (by default no filter)\", \n",
    "                    enum = [\"London\", \"Berlin\", \"Amsterdam\", \"Paris\"])\n",
    "\n",
    "@tool(args_schema=Filters)\n",
    "def get_monthly_active_users(month: str, city: str = None) -> int:\n",
    "    \"\"\"Returns number of active customers for the specified month\"\"\"\n",
    "    dt = datetime.datetime.strptime(month, '%Y-%m-%d')\n",
    "    total = dt.year + 10*dt.month\n",
    "    if city is None:\n",
    "        return total\n",
    "    else:\n",
    "        return int(total*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc48099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Wikipedia package to allow the LLM to look up information it needs:\n",
    "\n",
    "# !pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "class Wikipedia(BaseModel):\n",
    "    term: str = Field(description=\"Term to search for\")\n",
    "\n",
    "@tool(args_schema=Wikipedia)\n",
    "def get_summary(term: str) -> str:\n",
    "    \"\"\"Returns basic knowledge about the given term provided by Wikipedia\"\"\"\n",
    "    return wikipedia.summary(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62696b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s define a dictionary with all the functions our model knows now. This dictionary will help us to do routing later.\n",
    "\n",
    "toolkit = {\n",
    "    'percentage_difference': percentage_difference,\n",
    "    'get_monthly_active_users': get_monthly_active_users,\n",
    "    'get_summary': get_summary\n",
    "}\n",
    "\n",
    "analyst_functions = [format_tool_to_openai_function(f) \n",
    "  for f in toolkit.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force LLM to consult with Wikipedia if it needs some basic knowledge.\n",
    "# Changed the model to GPT 4 because it’s better for handling tasks requiring reasoning.\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview')\\\n",
    "  .bind(functions = analyst_functions)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. \\\n",
    "        You use only information provided in the initial request. \\\n",
    "        If you need to determine some information i.e. what is the name of the capital, you can use Wikipedia.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "print(analyst_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a42c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can invoke our chain with all the functions. Let’s start with a pretty straightforward query.\n",
    "# Turn off debug mode\n",
    "\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"How many users were in April 2023 from Berlin?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3427600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s try to make task a bit more complex by not telling the LLM what the capitol of Botswana is:\n",
    "\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03666862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now the model has to do a few things:\n",
    "# call Wikipedia to get the capital of Botswana\n",
    "# call the get_monthly_active_users function twice to get MAU for April and May\n",
    "# call percentage_difference to calculate the difference between metrics.\n",
    "\n",
    "observation1 = toolkit[result1.tool](result1.tool_input)\n",
    "print(observation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1)])\n",
    "})\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba310050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model wants to execute get_monthly_active_users with arguments {'month': '2023-04-01', 'city': 'Berlin'}. Let's do it and return the information to the model again:\n",
    "\n",
    "observation2 = toolkit[result2.tool](result2.tool_input)\n",
    "\n",
    "print(observation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1), (result2, observation2)])\n",
    "})\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ed14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation3 = toolkit[result3.tool](result3.tool_input)\n",
    "\n",
    "print(observation3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e351c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3)])\n",
    "})\n",
    "\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation4 = toolkit[result4.tool](result4.tool_input)\n",
    "print(observation4)\n",
    "\n",
    "result5 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3), (result4, observation4)])\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6f3a8",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = result5.return_values['output']\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
