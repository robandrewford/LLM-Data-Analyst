{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee571e87",
   "metadata": {},
   "source": [
    "### Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db57748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# %pip install openai==0.28\n",
    "# %pip install --upgrade langchain\n",
    "# %pip install --upgrade pydantic\n",
    "# %pip install --upgrade typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9be99f26-d5df-4a87-b568-9f8cbca3f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import langchain\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43d108b5-d04b-4f6f-84b0-1039f0e44cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store OpenAI API key in the venv\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('/Users/robertford/Documents/OpenAI-API/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "    for key in config:\n",
    "        os.environ['OPENAI_API_KEY'] = config[key]\n",
    "\n",
    "api_key=os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# print(os.environ['OPENAI_API_KEY']) # Check that it imported the key ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc8676-6142-4411-89ad-8b4da3e72948",
   "metadata": {},
   "source": [
    "### Extract information into a structured output using a Pydantic (data validation) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4dd4c8d-9b2c-458b-97ed-da73217e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class inheriting from the BaseModel class and define all the fields (arguments):\n",
    "\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class RequestStructure(BaseModel):\n",
    "  \"\"\"extracts information\"\"\"\n",
    "  metric: str = Field(description = \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\")\n",
    "  filters: Optional[str] = Field(description = \"filters to apply to the calculation (do not include filters on dates here)\")\n",
    "  dimensions: Optional[str] = Field(description = \"parameters to split you metric by\")\n",
    "  period_start: Optional[str] = Field(description = \"start day of the period for report\")\n",
    "  period_end: Optional[str] = Field(description = \"end day of the period for report\")\n",
    "  output_type: Optional[str] = Field(description = \"the desired output\", enum = [\"number\", \"visualisation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15b52e93-ef79-4cba-a54a-0be03253653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain to convert this Pydantic class into an OpenAI function:\n",
    "\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "extract_info_function = convert_pydantic_to_openai_function(RequestStructure, name = 'extract_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4181e0ed-5331-4c6c-aaa7-bcc51810be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'extract_information',\n",
       " 'description': 'extracts information',\n",
       " 'parameters': {'title': 'RequestStructure',\n",
       "  'description': 'extracts information',\n",
       "  'type': 'object',\n",
       "  'properties': {'metric': {'title': 'Metric',\n",
       "    'description': \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\",\n",
       "    'type': 'string'},\n",
       "   'filters': {'title': 'Filters',\n",
       "    'description': 'filters to apply to the calculation (do not include filters on dates here)',\n",
       "    'type': 'string'},\n",
       "   'dimensions': {'title': 'Dimensions',\n",
       "    'description': 'parameters to split you metric by',\n",
       "    'type': 'string'},\n",
       "   'period_start': {'title': 'Period Start',\n",
       "    'description': 'start day of the period for report',\n",
       "    'type': 'string'},\n",
       "   'period_end': {'title': 'Period End',\n",
       "    'description': 'end day of the period for report',\n",
       "    'type': 'string'},\n",
       "   'output_type': {'title': 'Output Type',\n",
       "    'description': 'the desired output',\n",
       "    'enum': ['number', 'visualisation'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['metric']}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangChain validates the class needed to use OpenAI:\n",
    "\n",
    "extract_info_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "576e23fc-1ceb-43a2-9cdf-334a996a1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a LangChain chain\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information from the provided request. \\\n",
    "            Extract ONLY the information presented in the initial request. \\\n",
    "            Don't add anything else. \\\n",
    "            Return partial information if something is missing.\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [extract_info_function])\n",
    "\n",
    "extraction_chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8939e35f-620f-4af8-afff-dcc2cf8418ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'extract_information', 'arguments': '{\"metric\":\"number of customers\",\"filters\":\"iOS\",\"dimensions\":\"country\",\"period_start\":\"April 1, 2023\",\"period_end\":\"April 30, 2023\",\"output_type\":\"number\"}'}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke 'request' defined in the chain into a structured ouput:\n",
    "\n",
    "extraction_chain.invoke({'request': \"How many customers visited our site on iOS in April 2023 from different countries?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01dbf58-52a2-46af-b4a6-13a4ab76fa63",
   "metadata": {},
   "source": [
    "### Define a tool that performs an explicit calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15ad8b00-e29c-426d-8d5c-688c17b938b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teach the LLM data analyst to calculate the difference between two metrics using an explicit calculation.\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a0a6325-7cc0-43e9-a166-7251363791bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage_difference\n",
      "{'metric1': {'title': 'Metric1', 'type': 'number'}, 'metric2': {'title': 'Metric2', 'type': 'number'}}\n",
      "percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics\n"
     ]
    }
   ],
   "source": [
    "# Now the function has the name and description parameters to be passed to the LLM:\n",
    "print(percentage_difference.name)\n",
    "print(percentage_difference.args)\n",
    "print(percentage_difference.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c682f11-2fbc-4237-ab2d-70f4716bd401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'percentage_difference',\n",
       " 'description': 'percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics',\n",
       " 'parameters': {'title': 'percentage_differenceSchemaSchema',\n",
       "  'type': 'object',\n",
       "  'properties': {'metric1': {'title': 'Metric1', 'type': 'number'},\n",
       "   'metric2': {'title': 'Metric2', 'type': 'number'}},\n",
       "  'required': ['metric1', 'metric2']}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the above parameters to create an OpenAI function spec:\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "# format_tool_to_openai_function(percentage_difference)\n",
    "format_tool_to_openai_function(percentage_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70958fde-3dcf-4d6f-974b-631c12c9c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Pydantic to specify a schema for the arguments:\n",
    "import pydantic.v1\n",
    "from typing import Type\n",
    "\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from pydantic.v1 because there is an error if using just pydantic\n",
    "# Use Pyndantic to specify a schema:\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "class Metrics(BaseModel):\n",
    "    metric1: float = Field(description=\"Base metric value to calculate the difference\")\n",
    "    metric2: float = Field(description=\"New metric value that we compare with the baseline\")\n",
    "\n",
    "@tool(args_schema=Metrics)\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd89c4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='percentage_difference', description='percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics', args_schema=<class '__main__.Metrics'>, func=<function percentage_difference at 0x14406b920>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2640cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":95}'}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a tool in practice to defining a chain and passing our tool to the function\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts.\"),\n",
    "    (\"user\", \"{request}\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model\n",
    "analyst_chain.invoke({'request': \"In April we had 100 users and in May only 95. What is difference in percent?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7195d1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='percentage_difference', tool_input={'metric1': 100, 'metric2': 110}, log=\"\\nInvoking: `percentage_difference` with `{'metric1': 100, 'metric2': 110}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}})])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = analyst_chain.invoke({'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92f69594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Execute the function as the LLM requested like this\n",
    "\n",
    "observation = percentage_difference(result.tool_input)\n",
    "print(observation)\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36934f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Define a message list to pass to the model observations to get a final answer from the model:\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "\n",
    "observation = percentage_difference(result1.tool_input)\n",
    "print(observation)\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a920ff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}}),\n",
       " FunctionMessage(content='10.0', name='percentage_difference')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, we need to add the observation to our observations variable. We could use format_to_openai_functions function to format our results in an expected way for the model.\n",
    "\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5140f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\",\n",
      "          \"additional_kwargs\": {\n",
      "            \"function_call\": {\n",
      "              \"name\": \"percentage_difference\",\n",
      "              \"arguments\": \"{\\\"metric1\\\":100,\\\"metric2\\\":110}\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"FunctionMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"name\": \"percentage_difference\",\n",
      "          \"content\": \"10.0\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\\nHuman: There were 100 users in April and 110 users in May. How did the number of users changed?\\nAI: {'name': 'percentage_difference', 'arguments': '{\\\"metric1\\\":100,\\\"metric2\\\":110}'}\\nFunction: 10.0\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The number of users increased by 10%.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The number of users increased by 10%.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 186,\n",
      "      \"completion_tokens\": 10,\n",
      "      \"total_tokens\": 196\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_cbe4fa03fe\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"agent\",\n",
      "    \"AgentFinish\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"return_values\": {\n",
      "      \"output\": \"The number of users increased by 10%.\"\n",
      "    },\n",
      "    \"log\": \"The number of users increased by 10%.\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.10s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "return_values={'output': 'The number of users increased by 10%.'} log='The number of users increased by 10%.'\n"
     ]
    }
   ],
   "source": [
    "# Let’s invoke our chain one more time, passing the function execution result as an observation.\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation)])\n",
    "})\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00432203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s add a couple more tools to our analyst’s toolkit,\n",
    "# using Pydantic to specify the input arguments for our function.\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "class Filters(BaseModel):\n",
    "    month: str = Field(description=\"Month of customer's activity in the format %Y-%m-%d\")\n",
    "    city: Optional[str] = Field(description=\"City of residence for customers (by default no filter)\", \n",
    "                    enum = [\"London\", \"Berlin\", \"Amsterdam\", \"Paris\"])\n",
    "\n",
    "@tool(args_schema=Filters)\n",
    "def get_monthly_active_users(month: str, city: str = None) -> int:\n",
    "    \"\"\"Returns number of active customers for the specified month\"\"\"\n",
    "    dt = datetime.datetime.strptime(month, '%Y-%m-%d')\n",
    "    total = dt.year + 10*dt.month\n",
    "    if city is None:\n",
    "        return total\n",
    "    else:\n",
    "        return int(total*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0cc48099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Wikipedia package to allow the LLM to look up information it needs:\n",
    "\n",
    "# !pip install wikipedia\n",
    "import wikipedia\n",
    "\n",
    "class Wikipedia(BaseModel):\n",
    "    term: str = Field(description=\"Term to search for\")\n",
    "\n",
    "@tool(args_schema=Wikipedia)\n",
    "def get_summary(term: str) -> str:\n",
    "    \"\"\"Returns basic knowledge about the given term provided by Wikipedia\"\"\"\n",
    "    return wikipedia.summary(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62696b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s define a dictionary with all the functions our model knows now. This dictionary will help us to do routing later.\n",
    "\n",
    "toolkit = {\n",
    "    'percentage_difference': percentage_difference,\n",
    "    'get_monthly_active_users': get_monthly_active_users,\n",
    "    'get_summary': get_summary\n",
    "}\n",
    "\n",
    "analyst_functions = [format_tool_to_openai_function(f) \n",
    "  for f in toolkit.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5085b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['observations', 'request'], input_types={'observations': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a product analyst willing to help your product team. You are very strict to the point and accurate.         You use only information provided in the initial request.         If you need to determine some information i.e. what is the name of the capital, you can use Wikipedia.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['request'], template='{request}')), MessagesPlaceholder(variable_name='observations')]) middle=[RunnableBinding(bound=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-1106-preview', temperature=0.1, openai_api_key='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', openai_proxy=''), kwargs={'functions': [{'name': 'percentage_difference', 'description': 'percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics', 'parameters': {'title': 'Metrics', 'type': 'object', 'properties': {'metric1': {'title': 'Metric1', 'description': 'Base metric value to calculate the difference', 'type': 'number'}, 'metric2': {'title': 'Metric2', 'description': 'New metric value that we compare with the baseline', 'type': 'number'}}, 'required': ['metric1', 'metric2']}}, {'name': 'get_monthly_active_users', 'description': 'get_monthly_active_users(month: str, city: str = None) -> int - Returns number of active customers for the specified month', 'parameters': {'title': 'Filters', 'type': 'object', 'properties': {'month': {'title': 'Month', 'description': \"Month of customer's activity in the format %Y-%m-%d\", 'type': 'string'}, 'city': {'title': 'City', 'description': 'City of residence for customers (by default no filter)', 'enum': ['London', 'Berlin', 'Amsterdam', 'Paris'], 'type': 'string'}}, 'required': ['month']}}, {'name': 'get_summary', 'description': 'get_summary(term: str) -> str - Returns basic knowledge about the given term provided by Wikipedia', 'parameters': {'title': 'Wikipedia', 'type': 'object', 'properties': {'term': {'title': 'Term', 'description': 'Term to search for', 'type': 'string'}}, 'required': ['term']}}]})] last=OpenAIFunctionsAgentOutputParser()\n"
     ]
    }
   ],
   "source": [
    "# Force LLM to consult with Wikipedia if it needs some basic knowledge.\n",
    "# Changed the model to GPT 4 because it’s better for handling tasks requiring reasoning.\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview')\\\n",
    "  .bind(functions = analyst_functions)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. \\\n",
    "        You use only information provided in the initial request. \\\n",
    "        If you need to determine some information i.e. what is the name of the capital, you can use Wikipedia.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "print(analyst_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4a42c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='get_monthly_active_users' tool_input={'month': '2023-04-01', 'city': 'Berlin'} log=\"\\nInvoking: `get_monthly_active_users` with `{'month': '2023-04-01', 'city': 'Berlin'}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_monthly_active_users', 'arguments': '{\"month\":\"2023-04-01\",\"city\":\"Berlin\"}'}})]\n"
     ]
    }
   ],
   "source": [
    "# We can invoke our chain with all the functions. Let’s start with a pretty straightforward query.\n",
    "# Turn off debug mode\n",
    "\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"How many users were in April 2023 from Berlin?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3427600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_summary', tool_input={'term': 'Capital of Botswana'}, log=\"\\nInvoking: `get_summary` with `{'term': 'Capital of Botswana'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_summary', 'arguments': '{\"term\":\"Capital of Botswana\"}'}})])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s try to make task a bit more complex by not telling the LLM what the capitol of Botswana is:\n",
    "\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": []\n",
    "})\n",
    "\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03666862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaborone (UK:  GAB-ə-ROH-nee, HAB-, US:  GAH-bə-ROH-nee, -⁠nay, Tswana: [χabʊˈrʊnɛ]) is the capital and largest city of Botswana with a population of 246,325 based on the 2022 census, about 10% of the total population of Botswana. Its agglomeration is home to 421,907 inhabitants at the 2011 census.\n",
      "Gaborone is situated between Kgale Hill and Oodi Hill, near the confluence of the Notwane River and Segoditshane River in the south-eastern corner of Botswana, 15 kilometres (9.3 mi) from the South African border. The city is served by the Sir Seretse Khama International Airport. It is an administrative district in its own right, but is the capital of the surrounding South-East District. Locals often refer to the city as GC or Motse-Mshate.The city of Gaborone is named after Chief Gaborone of the Tlokwa tribe, who once controlled land nearby. Because it had no tribal affiliation and was close to fresh water, the city was planned to be the capital in the mid-1960s when the Bechuanaland Protectorate became an independent nation. The centre of the city is a long strip of commercial businesses, called \"Main Mall\" with a semicircle-shaped area of government offices to its east. Gaborone was once one of the fastest-growing cities in the world, and this has created problems with housing and illegal settlements. The city has also dealt with conflicts spilling into the country from Zimbabwe and South Africa during the 1980s.\n",
      "Gaborone is the economic capital as well as the government capital; it is headquarters to numerous companies and the Botswana Stock Exchange. Gaborone is also home to the Southern African Development Community (SADC), a regional economic community established in 1980. Many languages are spoken there, Setswana (Tswana) being the main tongue. English, Kalanga, and Kgalagadi are also spoken.\n"
     ]
    }
   ],
   "source": [
    "# So now the model has to do a few things:\n",
    "# call Wikipedia to get the capital of Botswana\n",
    "# call the get_monthly_active_users function twice to get MAU for April and May\n",
    "# call percentage_difference to calculate the difference between metrics.\n",
    "\n",
    "observation1 = toolkit[result1.tool](result1.tool_input)\n",
    "print(observation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f15b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='get_monthly_active_users' tool_input={'month': '2023-04-01', 'city': 'Gaborone'} log=\"\\nInvoking: `get_monthly_active_users` with `{'month': '2023-04-01', 'city': 'Gaborone'}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_monthly_active_users', 'arguments': '{\"month\":\"2023-04-01\",\"city\":\"Gaborone\"}'}})]\n"
     ]
    }
   ],
   "source": [
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1)])\n",
    "})\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba310050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# The model wants to execute get_monthly_active_users with arguments {'month': '2023-04-01', 'city': 'Berlin'}. Let's do it and return the information to the model again:\n",
    "\n",
    "observation2 = toolkit[result2.tool](result2.tool_input)\n",
    "\n",
    "print(observation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d81c3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='get_monthly_active_users' tool_input={'month': '2023-05-01', 'city': 'Gaborone'} log=\"\\nInvoking: `get_monthly_active_users` with `{'month': '2023-05-01', 'city': 'Gaborone'}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_monthly_active_users', 'arguments': '{\"month\":\"2023-05-01\",\"city\":\"Gaborone\"}'}})]\n"
     ]
    }
   ],
   "source": [
    "result3 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1), (result2, observation2)])\n",
    "})\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c7ed14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    }
   ],
   "source": [
    "observation3 = toolkit[result3.tool](result3.tool_input)\n",
    "\n",
    "print(observation3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e351c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool='percentage_difference' tool_input={'metric1': 5, 'metric2': 533} log=\"\\nInvoking: `percentage_difference` with `{'metric1': 5, 'metric2': 533}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":5,\"metric2\":533}'}})]\n"
     ]
    }
   ],
   "source": [
    "result4 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3)])\n",
    "})\n",
    "\n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a054ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10560.0\n"
     ]
    }
   ],
   "source": [
    "observation4 = toolkit[result4.tool](result4.tool_input)\n",
    "print(observation4)\n",
    "\n",
    "result5 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Botswana change between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3), (result4, observation4)])\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6f3a8",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17a5724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users from Gaborone, the capital of Botswana, increased by 10,560% between April and May 2023.\n"
     ]
    }
   ],
   "source": [
    "output = result5.return_values['output']\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
